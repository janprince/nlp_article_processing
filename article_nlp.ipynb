{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Getting Human Language Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we use an article on Nikola Tesla from history.com"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def request():\n",
    "    url = \"https://www.history.com/topics/inventions/nikola-tesla\"\n",
    "    r = requests.get(url)\n",
    "    text = r.text\n",
    "\n",
    "    # create a beautiful soup object\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    return soup\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def scrape_info(soup):\n",
    "    \"\"\"\n",
    "    :param soup: Beautiful soup object\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    article_title = soup.find(\"h1\", class_=\"m-detail-header--title\")\n",
    "    article_title = article_title.text\n",
    "\n",
    "    article_body = soup.find(\"div\", class_=\"m-detail--body\")\n",
    "    article_body.find(\"aside\").decompose()\n",
    "    article_body = article_body.text\n",
    "\n",
    "    return article_title, article_body\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import nltk\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Get data\n",
    "title, article = scrape_info(request())\n",
    "\n",
    "# sent_tokenize() to split up article into sentences:\n",
    "sentences = sent_tokenize(article)\n",
    "\n",
    "# tokenizing by word\n",
    "article_words = word_tokenize(article)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "['Serbian-American engineer and physicist Nikola Tesla (1856-1943) made dozens of breakthroughs in the production, transmission and application of electric power.',\n 'He invented the first alternating current (AC) motor and developed AC generation and transmission technology.',\n 'Though he was famous and respected, he was never able to translate his copious inventions into long-term financial success—unlike his early employer and chief rival, Thomas Edison.Nikola Tesla’s Early Years     Nikola Tesla was born in 1856 in Smiljan, Croatia, then part of the Austro-Hungarian Empire.']"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering Stop Words\n",
    "Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# a list of english stop words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# filter word tokens\n",
    "filtered_words = [word for word in article_words if word not in stop_words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing Punctuations\n",
    "\n",
    "Punctuations such as .,? must be removed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "['engineer',\n 'physicist',\n 'Nikola',\n 'Tesla',\n 'made',\n 'dozens',\n 'breakthroughs',\n 'production',\n 'transmission',\n 'application']"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_article_words = [word for word in filtered_words if word.isalpha()]\n",
    "filtered_article_words[: 10]\n",
    "\n",
    "# What remains are words where all characters are alphabet letters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.” Stemming allows you to zero in on the basic meaning of a word rather than all the details of how it’s being used. NLTK has more than one stemmer, but you’ll be using the Porter stemmer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "['serbian-american', 'engin', 'physicist', 'nikola', 'tesla', '(', '1856-1943']"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# stemming object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stemmed_words\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "stemmed_words[:7]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tagging Parts of Speech\n",
    "\n",
    "Part of speech is a grammatical term that deals with the roles words play when you use them together in sentences. Tagging parts of speech, or POS tagging, is the task of labeling the words in your text according to their part of speech.\n",
    "\n",
    "Here’s how to import the relevant parts of NLTK in order to tag parts of speech:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Serbian-American', 'JJ'),\n ('engineer', 'NN'),\n ('physicist', 'NN'),\n ('Nikola', 'NNP'),\n ('Tesla', 'NNP')]"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "article_pos = nltk.pos_tag(filtered_words)\n",
    "article_pos[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the words in the quote are now in a separate tuple, with a tag that represents their part of speech. But what do the tags mean? Here’s how to get a list of tags and their meanings:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Now that you’re up to speed on parts of speech, you can circle back to lemmatizing. Like stemming, lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like 'discoveri'.\n",
    "\n",
    "**Note:** A lemma is a word that represents a whole group of words, and that group of words is called a lexeme.\n",
    "\n",
    "For example, if you were to look up the word “blending” in a dictionary, then you’d need to look at the entry for “blend,” but you would find “blending” listed in that entry.\n",
    "\n",
    "In this example, “blend” is the lemma, and “blending” is part of the lexeme. So when you lemmatize a word, you are reducing it to its lemma.\n",
    "\n",
    "Here’s how to import the relevant parts of NLTK in order to start lemmatizing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "['Serbian-American', 'engineer', 'physicist', 'Nikola', 'Tesla']"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "lemmatized_words[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chunking\n",
    "\n",
    "While tokenizing allows you to identify words and sentences, chunking allows you to identify phrases.\n",
    "\n",
    "Note: A phrase is a word or group of words that works as a single unit to perform a grammatical function. Noun phrases are built around a noun.\n",
    "\n",
    "**Here are some examples:**\n",
    "\n",
    "“A planet”\n",
    "“A tilting planet”\n",
    "“A swiftly tilting planet”\n",
    "\n",
    "Chunking makes use of POS tags to group words and apply chunk tags to those groups. Chunks don’t overlap, so one instance of a word can be in only one chunk at a time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You’ve got a list of tuples of all the words in the quote, along with their POS tag. In order to chunk, you first need to define a **chunk grammar**.\n",
    "\n",
    "**Note:** A chunk grammar is a combination of rules on how sentences should be chunked. It often uses regular expressions, or regexes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# chunk grammar with regular exp\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "According to the rule you created, your chunks:\n",
    "\n",
    "Start with an optional (?) determiner ('DT')\n",
    "Can have any number (*) of adjectives (JJ)\n",
    "End with a noun (<NN>)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# chunk_parser\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "tree = chunk_parser.parse(nltk.pos_tag(article_words))\n",
    "\n",
    "tree.draw()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Named Entity Recognition (NER)\n",
    "Named entities are noun phrases that refer to specific locations, people, organizations, and so on. With named entity recognition, you can find the named entities in your texts and also determine what kind of named entity they are."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "tree = nltk.ne_chunk(article_pos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "tree.draw()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frequency Distribution\n",
    "With a frequency distribution, you can check which words show up most frequently in your text. You’ll need to get started with an import:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "FreqDist({'Tesla': 27, 'Edison': 8, 'AC': 7, 'first': 6, 'Westinghouse': 6, 'power': 5, 'He': 5, 'In': 5, 'years': 5, 'New': 5, ...})"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(filtered_article_words)\n",
    "fdist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "FreqDist is a subclass of collections.Counter. Here’s how to create a frequency distribution of the entire corpus of personals ads:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Tesla', 27),\n ('Edison', 8),\n ('AC', 7),\n ('first', 6),\n ('Westinghouse', 6),\n ('power', 5),\n ('He', 5),\n ('In', 5),\n ('years', 5),\n ('New', 5)]"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The top ten most frequent words in the article\n",
    "top_ten_words = fdist.most_common(10)\n",
    "\n",
    "top_ten_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}